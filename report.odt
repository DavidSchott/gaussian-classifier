David Schott s1329380
					Inf2B Natural images classification Report
									KNN Analysis:
K-nearest-neighbours 5 confusion matrix with accuracy 56%

2986
305
291
87
162
72
152
165
498
282
166
3511
79
55
76
53
147
100
223
590
279
124
2151
288
607
343
643
361
102
102
82
128
295
1656
457
986
814
342
75
165
100
58
442
235
2388
271
643
706
57
100
63
78
256
700
439
2268
594
479
30
93
40
51
286
160
366
163
3765
100
19
50
56
65
178
190
524
282
234
3295
20
156
543
497
115
79
74
48
73
76
3160
335
212
719
81
104
138
102
152
250
203
3039

The classification accuracy for knn-5 was about 56%. There were similar values for knn-1 and knn-3. This is because the knn-classification is prone to misclassification whenever there are classes with a similar image. For example, in the confusion matrix above class 4 (Cat) was classified correctly only 1656/5000 times, i.e. 33% accuracy. Interestingly, 986 cat-images were misclassified as Class 6 (Dog), which is almost 20%.Similarly, we see that when classifying class 6 or dogs, 700 dogs were classified as cats, and again accuracy was only 2268/5000, or 45%. What I found noteworthy was that the running time were quite different, yet there was no notable improvement between accuracies of knn1,knn3 or knn5. Knn5 takes about 65 minutes to run, whereas knn1 runs almost twice as fast at about 35-40 min. Thus, if efficiency is not a large priority, and the dataset is of a similar size, knn1 is a preferable approach. Overall, class 7 (frog) was the easiest to classify (75%)  because it is very distinct.

						Gaussian Unique Covariance Analysis:
Full Gaussian Confusion Matrix  with Accuracy 0.65

3160
178
279
326
48
62
20
52
598
277
107
3817
89
145
51
41
30
33
219
468
353
92
2292
702
469
333
418
178
101
62
75
79
279
3036
142
874
234
89
74
118
185
45
177
550
2889
217
311
406
116
104
21
38
226
1263
287
2733
153
176
49
54
49
57
286
460
189
245
3609
35
33
37
56
49
148
470
347
346
28
3352
55
149
228
259
95
196
18
38
12
15
3850
289
131
388
80
200
47
55
29
64
151
3855



David Schott s1329380
Here we used a full covariance Gaussian model with prior probability 1/10 for each class, and quadratic classification boundaries due to unique covariance matrices. Thus if we would choose a feature vector at random, the classification accuracy would be 10%.  The classification accuracy for the full Gaussian confusion matrix was about 65%. Here class 4 and class 6 had better classifications than k-nearest-neighbors. In class 4, 17% cats were misclassified as dogs, and the total accuracy of class 4 was 60%. This means that Gaussian classification is less prone to misclassification for similar datasets. Interestingly, the lowest accuracy was encountered in class 3 (bird), with only 2295/5000 = 45% accuracy. This was mainly because 1171 birds were classified as either cat or deer. Gaussian classification takes about 50-60 minutes to complete classifying the data. Also, Gaussian classification was most reliable for all the classes for transportation (airplane,automobile,truck,ship), with a good accuracy of 77% for both ship and truck.

							Gaussian Shared Covariance
Shared Covariance Confusion Matrix Accuracy 68%
3452
176
316
117
90
48
43
98
429
231
172
3932
60
79
32
13
75
50
124
463
343
47
2731
361
532
301
368
197
77
43
88
53
365
2683
245
901
383
130
55
97
142
19
307
316
3145
102
457
512
62
48
26
38
345
925
298
2886
154
266
24
38
30
37
297
290
247
137
3883
38
22
19
55
18
213
233
355
375
45
3580
36
90
329
246
94
72
42
23
45
29
3924
196
168
452
59
90
46
26
40
50
122
3947


Here we used a Gaussian Model with a general covariance matrix belonging to each class. Interestingly, the shared covariance confusion matrix returns the best accuracy. This seems quite counter-intuitive, and is probably an exception or an error. The runtime is roughly the same as with the full covariance approach, because I have not optimized it to remove redundant, but equivalent calculations. The results correlate with the full covariance Gaussian model, except for class 4, which is an outlier.

								Recommendation

Summarizing, we can see that when trying to classify larger datasets, Gaussian classification is the preferred application method, because k-nearest neighbours suffers from the curse of dimensionality and lower reliability amongst similar classes. Nonetheless, if the dataset has more distinct decision regions, and is small, k-nearest neighbours works faster and is similarly reliable (i.e. Frog classification of 75%).
